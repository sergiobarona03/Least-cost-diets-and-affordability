# Cargar mapa
# Leer dpto_shape
setwd("C:/Users/Portatil/Desktop/Spatial-Econometrics-using-R/")
# Unir las bases de datos
mun_shape <- st_read(dsn = "SpatialData\\DANE_geodata\\",
layer = "MGN_ANM_MPIOS") %>% mutate(
cod = as.numeric(
paste0(DPTO_CCDGO, MPIO_CCDGO))
)
# Unir las bases de datos
mun_merged <-  mun_shape[c("cod",
"geometry")] %>%
left_join(mun.data,by = c("cod" = "cod"))
# Figura 3:
library(viridis)
ggplot(data = mun_merged) +
geom_sf(aes(fill = tasa_nbi), color = "black", size = 0.2) +
scale_fill_gradientn(colors = c("#1a9850", "#fee08b", "#d73027"),
values = c(0, 0.5, 1),
name = "Tasa NBI (%)") +
theme_bw() +
labs(title = "Distribucion espacial del NBI por Municipios (% en NBI)",
caption = "Fuente: DANE. CNPV-2018.") +
theme(axis.text = element_blank(),
axis.ticks = element_blank(),
panel.grid = element_blank(),
legend.position = "right",
legend.background = element_rect(fill = "white", color = "black"))
# Figura 4:
mun_merged$tasa_nbi_cat <- cut(mun_merged$tasa_nbi,
breaks = c(0, 10, 20, 30, 40, 50, Inf),
labels = c("0-10%", "10-20%",
"20-30%", "30-40%",
"40-50%", "50%+"),
include.lowest = TRUE)
colors_intervals <- c("0-10%" = "#1a9850",  # Verde
"10-20%" = "#66bd63", # Verde claro
"20-30%" = "#fee08b", # Amarillo-naranja
"30-40%" = "#fdae61", # Naranja
"40-50%" = "#f46d43", # Rojo claro
"50%+" = "#d73027")   # Rojo fuerte
map_nbi <- ggplot(data = mun_merged) +
geom_sf(aes(fill = tasa_nbi_cat), color = "black", size = 0.2) +
scale_fill_manual(values = colors_intervals, name = "Tasa NBI (%)") +
theme_bw() +
labs(title = "Distribucion espacial del NBI por Municipios (en % de NBI)",
caption = "Fuente: DANE. CNPV-2018.") +
theme(axis.text = element_blank(),
axis.ticks = element_blank(),
panel.grid = element_blank(),
legend.position = "bottom",
legend.background = element_rect(fill = "white", color = "black"))
map_nbi
mun_merged <- mun_merged[-c(629, 630),]
# Para el análisis, se eliminan los valores faltantes
mun_merged2 = mun_merged %>% filter(!is.na(corr_tot) &
!is.na(n) & !is.na(area_km) &
!is.na(ocup_rate))
# Logaritmo natural de la tasa
mun_merged2$log_nbi <- log(mun_merged2$tasa_nbi)
# Se propone una regresión lineal múltiple con las siguientes variables
reg_mco <- lm(log_nbi ~  n + corr_tot + log(area_km) + ocup_rate, data=mun_merged2)
summary(reg_mco)
reg_mco_res <- reg_mco$residuals
# A continuación examinamos los residuos
me1 <- mean(residuals(reg_mco))
me1
sd1 <- sd(residuals(reg_mco))
sd1
summary(residuals(reg_mco))  # Simetría razonable
hist(residuals(reg_mco),  breaks=seq(-2, 4.2, 0.1), col=8, probability=T,
ylab='Density', main='Histogram of Residuals(reg_mco)',
xlab='Residuals(reg_mco)')
box()
curve(dnorm(x, mean=me1, sd=sd1), from=-2, to=4.2, add=T,
col='red', lwd=2)
# Para el análisis, se eliminan los valores faltantes
mun_merged2 = mun_merged %>% filter(!is.na(corr_tot) &
!is.na(n) & !is.na(area_km) &
!is.na(ocup_rate))
# Logaritmo natural de la tasa
mun_merged2$log_nbi <- log(mun_merged2$tasa_nbi)
# Se propone una regresión lineal múltiple con las siguientes variables
reg_mco <- lm(log_nbi ~  n + corr_tot + log(area_km) + ocup_rate, data=mun_merged2)
summary(reg_mco)
reg_mco_res <- reg_mco$residuals
#---------------------------------#
# Diagnóstico de los residuales   #
#---------------------------------#
# A continuación examinamos los residuos
me1 <- mean(residuals(reg_mco))
me1
sd1 <- sd(residuals(reg_mco))
sd1
summary(residuals(reg_mco))  # Simetría razonable
hist(residuals(reg_mco),  breaks=seq(-2, 4.2, 0.1), col=8, probability=T,
ylab='Density', main='Histogram of Residuals(reg_mco)',
xlab='Residuals(reg_mco)')
box()
curve(dnorm(x, mean=me1, sd=sd1), from=-2, to=4.2, add=T,
col='red', lwd=2)
# Hay señales de ouliers. Veamos el QQ-plot
library(car)
car::qqPlot(residuals(reg_mco), distribution="norm",
xlab='', main='Quantile Comparison Plot reg_mco residuals',
envelope=.95, las=0, pch=NA, lwd=2, col="red",
line="quartiles")
par(new=TRUE)
car::qqPlot(residuals(reg_mco), distribution="norm", envelope=FALSE,
pch=1, cex=1, col="black")
par(new=FALSE)
# Nuestra intuición se verifica con numerosas pruebas de normalidad univariada
library(nortest)
library(tseries)
ad.test(residuals(reg_mco))
lillie.test(residuals(reg_mco))
pearson.test(residuals(reg_mco))
cvm.test(residuals(reg_mco))
sf.test(residuals(reg_mco))
shapiro.test(residuals(reg_mco))
# No hay normalidad en los residuos
# A partir del siguiente gráfico, estudiamos el supuesto de homocedasticidad
plot(fitted(reg_mco), residuals(reg_mco), xlab="Fitted y", ylab= "Residuals",
main="Plot of Residuals against Fitted y")
abline(h=0)
# Usemos una prueba formal:
library(lmtest)
bptest(reg_mco) # No se rechaza Ho (Homocedasticidad)
#----------------------------------------------#
# Se verifica la independencia de los residuos #
#----------------------------------------------#
# Se define nuevamente la matriz de pesos
nb_mun <- poly2nb(mun_merged2)
nb_mun
# Se crea la matriz W
nb2_mun<- nb2listw(nb_mun)
#----------------------------------------------#
# Se verifica la independencia de los residuos #
#----------------------------------------------#
# Se define nuevamente la matriz de pesos
nb_mun <- poly2nb(mun_merged2)
nb_mun
# Eliminar regiones sin enlaces
mun_merged2 <- mun_merged2[-c(202, 205),]
# Se crea la matriz W
nb2_mun<- nb2listw(nb_mun)
#----------------------------------------------#
# Se verifica la independencia de los residuos #
#----------------------------------------------#
# Se define nuevamente la matriz de pesos
nb_mun <- poly2nb(mun_merged2)
nb_mun
# Eliminar regiones sin enlaces
mun_merged2 <- mun_merged2[-c(202, 205),]
nb_mun <- poly2nb(mun_merged2)
nb_mun
# Se crea la matriz W
nb2_mun<- nb2listw(nb_mun)
summary(nb2_mun)
# Véase la información de los pesos de la matriz
names(attributes(nb2_mun))  # Nombres de los atributos
card(nb2_mun$neighbours)    # Número de vecinos para cada observación
range(card(nb2_mun$neighbours))  # Rango (número de vecinos)
1/rev(range(card(nb2_mun$neighbours))) # Rango (pesos)
summary(nb2_mun, zero.policy=T)   # Resumen
moran.test(reg_mco$residuals,
nb2_mun, alternative="two.sided", zero.policy=T)
# Para el análisis, se eliminan los valores faltantes
mun_merged2 = mun_merged %>% filter(!is.na(corr_tot) &
!is.na(n) & !is.na(area_km) &
!is.na(ocup_rate))
# Eliminar regiones sin enlaces
mun_merged2 <- mun_merged2[-c(202, 205),]
# Logaritmo natural de la tasa
mun_merged2$log_nbi <- log(mun_merged2$tasa_nbi)
# Se propone una regresión lineal múltiple con las siguientes variables
reg_mco <- lm(log_nbi ~  n + corr_tot + log(area_km) + ocup_rate, data=mun_merged2)
summary(reg_mco)
reg_mco_res <- reg_mco$residuals
#---------------------------------#
# Diagnóstico de los residuales   #
#---------------------------------#
# A continuación examinamos los residuos
me1 <- mean(residuals(reg_mco))
me1
sd1 <- sd(residuals(reg_mco))
sd1
summary(residuals(reg_mco))  # Simetría razonable
hist(residuals(reg_mco),  breaks=seq(-2, 4.2, 0.1), col=8, probability=T,
ylab='Density', main='Histogram of Residuals(reg_mco)',
xlab='Residuals(reg_mco)')
box()
curve(dnorm(x, mean=me1, sd=sd1), from=-2, to=4.2, add=T,
col='red', lwd=2)
# Hay señales de ouliers. Veamos el QQ-plot
library(car)
car::qqPlot(residuals(reg_mco), distribution="norm",
xlab='', main='Quantile Comparison Plot reg_mco residuals',
envelope=.95, las=0, pch=NA, lwd=2, col="red",
line="quartiles")
par(new=TRUE)
car::qqPlot(residuals(reg_mco), distribution="norm", envelope=FALSE,
pch=1, cex=1, col="black")
par(new=FALSE)
# Nuestra intuición se verifica con numerosas pruebas de normalidad univariada
library(nortest)
library(tseries)
ad.test(residuals(reg_mco))
lillie.test(residuals(reg_mco))
pearson.test(residuals(reg_mco))
cvm.test(residuals(reg_mco))
sf.test(residuals(reg_mco))
shapiro.test(residuals(reg_mco))
# No hay normalidad en los residuos
# A partir del siguiente gráfico, estudiamos el supuesto de homocedasticidad
plot(fitted(reg_mco), residuals(reg_mco), xlab="Fitted y", ylab= "Residuals",
main="Plot of Residuals against Fitted y")
abline(h=0)
# Usemos una prueba formal:
library(lmtest)
bptest(reg_mco) # No se rechaza Ho (Homocedasticidad)
#----------------------------------------------#
# Se verifica la independencia de los residuos #
#----------------------------------------------#
# Se define nuevamente la matriz de pesos
nb_mun <- poly2nb(mun_merged2)
nb_mun
# Se crea la matriz W
nb2_mun<- nb2listw(nb_mun)
summary(nb2_mun)
# Véase la información de los pesos de la matriz
names(attributes(nb2_mun))  # Nombres de los atributos
card(nb2_mun$neighbours)    # Número de vecinos para cada observación
range(card(nb2_mun$neighbours))  # Rango (número de vecinos)
1/rev(range(card(nb2_mun$neighbours))) # Rango (pesos)
summary(nb2_mun, zero.policy=T)   # Resumen
moran.test(reg_mco$residuals,
nb2_mun, alternative="two.sided", zero.policy=T)
# Seleccionar la paleta
colors <- brewer.pal(5, "YlOrBr")
color.cat.reg<-classIntervals(reg_mco$residuals, n=5, style="quantile", dataPrecision=2)
colcode <- findColours(color.cat.reg, colors)
## Figura
par(mfrow=c(1,1))
plot(mun_merged2[c("nom_mun", "geometry")], col=colcode)
title('Map of Regression Residuals')
legend('topleft', legend=c(names(attr(colcode, 'table'))), fill=c(attr(colcode, 'palette')),
title='Regression Residuals')
moran.plot(reg_mco$residuals, nb2_mun, zero.policy=T, labels=as.character(mun_merged2$nom_mun),
xlab=NULL, ylab=NULL, type="p", col="#AE017E",
cex=0.8, pch=1)
model.lag.eig <- spatialreg::lagsarlm(log_nbi ~  n + corr_tot + log(area_km) + ocup_rate,
data=mun_merged, nb2_mun, method="eigen", quiet=FALSE)
install.packages("shiny")
library(shiny)
ui <- fluidPage()
shinyApp(ui = ui, server = server)
server <- functioninput, output){
shinyApp(ui = ui, server = server)
}
server <- functioninput, output){
shinyApp(ui = ui, server = server)
}
server <- functioninput, output){
shinyApp(ui = ui, server = server)
}
server <- function(input, output){
shinyApp(ui = ui, server = server)
}
ui <- fluidPage(textInput(inputId = "name",
label = "Name"))
server <- function(input, output){
shinyApp(ui = ui, server = server)
}
ui <- fluidPage(textInput(inputId = "name",
label = "Name",
value = " ",
placeholder = "Lisa"))
server <- function(input, output){
shinyApp(ui = ui, server = server)
}
knitr::opts_chunk$set(echo = TRUE)
library(shiny)
ui <- fluidPage(textInput(inputId = "name",
label = "Name",
value = " ",
placeholder = "Lisa"))
server <- function(input, output){
shinyApp(ui = ui, server = server)
}
library(shiny)
ui <- fluidPage(textInput(inputId = "name",
label = "Name",
value = " ",
placeholder = "Lisa"))
server <- function(input, output){
shinyApp(ui = ui, server = server)
}
runApp('C:/Users/Portatil/Desktop/test')
input$municipality
# Define UI for application that draws a histogram
ui <- fluidPage(
# Application title
titlePanel("FoodpriceR: Pobreza alimentaria en Colombia"),
# Sidebar with a slider input for number of bins
selectInput("municipality",
"Select a municipality:",
choices = c("Bogotá", "Medellín", "Cali", "Barranquilla", "Cartagena",
"Cúcuta", "Bucaramanga", "Pereira", "Ibagué", "Pasto",
"Manizales", "Neiva", "Villavicencio", "Valledupar", "Armenia",
"Sincelejo", "Popayán", "Palmira", "Buenaventura", "Tuluá",
"Cartago", "Tunja", "Ipiales", "Montería"),
selected = "Bogotá")
plotOutput(print(input$municipality))
)
file.choose()
dataset=read.csv("C:\\Users\\Portatil\\Downloads\\agaricus-lepiota.data",
header = F,col.names=c("poiede",
"cap_shape",
"cap_surface",
"cap_color",
"bruises","odor",
"gill_attachment",
"gill_spacing",
"gill_size","gill_color",
"stalk_shape","stalk_root",
"stalk_surface_above_ring",
"stalk_surface_below_ring",
"stalk_color_above_ring",
"stalk_color_below_ring",
"veil_type","veil_color","ring_number",
"ring_type","spore_print_color","population","habitat"))
View(dataset)
levels(as.factor(dataset$poiede))
install.packages("keras")
install.packages("keras")
R.version
install.packages("keras")
library(keras)
library(reticulate)
reticulate::virtualenv_create("r-reticulate", python = install_python())
reticulate::virtualenv_create("r-reticulate",
python = install_python())
library(reticulate)
library(keras)install_keras(envname = "r-reticulate")
library(keras)
reticulate::virtualenv_create("r-reticulate",
python = install_python())
reticulate::py_discover_config()
library(reticulate)
reticulate::py_discover_config()
reticulate::virtualenv_create(
"r-reticulate",
python = reticulate::install_python())
reticulate::py_discover_config()
reticulate::py_discover_config()
reticulate::virtualenv_create(
"r-reticulate",
python = reticulate::install_python())
reticulate::virtualenv_create(
"r-reticulate",
python = reticulate::install_python())
getwd()
library(keras)
library(tensorflow)
library(tidyverse)
# Mnist data set
mnist <- keras::dataset_mnist()
install.packages("keras")
library(reticulate)
reticulate::virtualenv_create("r-reticulate", python = install_python())
Sys.which("git")
getwd()
Sys.which("git")
Sys.which("git")
reticulate::virtualenv_create("r-reticulate", python = install_python())
library(reticulate)
reticulate::virtualenv_create("r-reticulate", python = install_python())
getwd()
Sys.which("git")
library(reticulate)
reticulate::virtualenv_create("r-reticulate", python = install_python())
reticulate::virtualenv_create("r-reticulate",
python = install_python())
file.choose()
file.choose()
reticulate::virtualenv_create("r-reticulate",
python = install_python())
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
library(kableExtra)
library(readxl)
library(ape)
library(knitr)
library(geosphere)
library(sf)
# Cargar resultados
data.multilevel = readxl::read_excel("C:\\Users\\Portatil\\Desktop\\IPV-Code\\R-code\\multilevel-analysis\\input\\v2_multilevel_dataset.xlsx")
test = dataset %>% filter(!is.na(ipv))
test = data.multilevel %>% filter(is.na(ipv))
nrow(test)
556/25446
(556/25446)*100
levels(as.factor(data.multilevel$eth))
dplyr::count(data.multilevel, eth)
# Recodificación de la variable etnia
data.multilevel$eth = factor(data.multilevel$eth,
levels = 1:6,
labels = c("No", "No", "No", "No",
"No", "Yes"))
# Cargar resultados
data.multilevel = readxl::read_excel("C:\\Users\\Portatil\\Desktop\\IPV-Code\\R-code\\multilevel-analysis\\input\\v2_multilevel_dataset.xlsx")
# Provisionalmente se eliminan NAs en IPV
data.multilevel = data.multilevel %>% filter(!is.na(ipv))
# Supuesto: si "don't know", entonces "No" (Variable de testigo de violencia)
data.multilevel$viol_wtn[data.multilevel$viol_wtn == "DK"] = "No"
# Recodificación de la variable etnia
data.multilevel$eth = factor(data.multilevel$eth,
levels = 1:6,
labels = c("Yes", "Yes", "Yes", "Yes",
"Yes", "No"))
se <- sqrt(diag(vcov(m1)))
# Cargar resultados
data.multilevel = readxl::read_excel("C:\\Users\\Portatil\\Desktop\\IPV-Code\\R-code\\multilevel-analysis\\input\\v2_multilevel_dataset.xlsx")
# Provisionalmente se eliminan NAs en IPV
data.multilevel = data.multilevel %>% filter(!is.na(ipv))
# Supuesto: si "don't know", entonces "No" (Variable de testigo de violencia)
data.multilevel$viol_wtn[data.multilevel$viol_wtn == "DK"] = "No"
# Recodificación de la variable etnia
data.multilevel$eth = factor(data.multilevel$eth,
levels = 1:6,
labels = c("Yes", "Yes", "Yes", "Yes",
"Yes", "No"))
null_model <- glmer(ipv ~ 1 + (1 | codmpio) ,
data = dataset,
family = binomial,
control = glmerControl(optimizer = "bobyqa"),
nAGQ = 10)
library(lme4)
null_model <- glmer(ipv ~ 1 + (1 | codmpio) ,
data = dataset,
family = binomial,
control = glmerControl(optimizer = "bobyqa"),
nAGQ = 10)
library(lme4)
null_model <- glmer(ipv ~ 1 + (1 | codmpio) ,
data = data.multilevel,
family = binomial,
control = glmerControl(optimizer = "bobyqa"),
nAGQ = 10)
# Crear la variable de municipio
data.multilevel$depto = data.multilevel$dept*1000
data.multilevel$codmpio =data.multilevel$depto + dataset$muni
data.multilevel$codmpio =data.multilevel$depto + data.multilevel$muni
data.multilevel$codmpio
library(lme4)
null_model <- glmer(ipv ~ 1 + (1 | codmpio) ,
data = data.multilevel,
family = binomial,
control = glmerControl(optimizer = "bobyqa"),
nAGQ = 10)
summary(null_model)
library(sjPlot)
install.packages(sjPlot)
install.packages("sjPlot")
library(sjPlot)
m0 <- glmer(ipv ~ 1 + (1 | codmpio) ,
data = data.multilevel,
family = binomial,
control = glmerControl(optimizer = "bobyqa"),
nAGQ = 10)
summary(m0)
data.multilevel$eth
file.choose()
data = readRDS("C:\\Users\\Portatil\\Desktop\\informality-gap\\Input\\v4_output_imput.RDS")
levels(as.factor(data$rama_eco))
levels(as.factor(data$rama_eco))
dplyr::count(data, rama_eco)
dplyr::count(data, RAMA2D_R4)
library(haven)
dataset = haven::read_dta("v4_output_imput.dta")
setwd("C:\Users\Portatil\Desktop\informality-gap\Input")
dataset = haven::read_dta("v4_output_imput.dta")
setwd("C:\Users\Portatil\Desktop\informality-gap\Input'")
setwd("C:\Users\Portatil\Desktop\informality-gap\Input\")
dataset = haven::read_dta("v4_output_imput.dta")
setwd("C:\Users\Portatil\Desktop\informality-gap\Input\")
dataset = haven::read_dta("v4_output_imput.dta")
setwd("C:\Users\Portatil\Desktop\informality-gap\Input\")
dataset = haven::read_dta("v4_output_imput.dta")
setwd("C:\Users\Portatil\Desktop\informality-gap\Input\)
dataset = haven::read_dta("v4_output_imput.dta")
setwd("C:\Users\Portatil\Desktop\informality-gap\Input")
file.choose()
file.choose()
x ="C:\\Users\\Portatil\\Desktop\\Least-cost-diets-and-affordability\\Proyecto Interno\\estimadores-banrep\\CALI\\estimadores-DANE\\input\\010925_retail_price_data.csv"
setwd("C:\\Users\\Portatil\\Desktop\\Least-cost-diets-and-affordability\\Proyecto Interno\\estimadores-banrep\\CALI\\estimadores-DANE\\input\\")
View(C:\\Users\\Portatil\\Desktop\\Least-cost-diets-and-affordability\\Proyecto Interno\\estimadores-banrep\\CALI\\estimadores-DANE\\input\\)
x = read.csv("010925_retail_price_data.csv")
View(x)
head(x)
x = x %>% select(ano, mes_num, cod_mun, nombre_ciudad, codigo_articulo, articulo)
library(tidyverse)
x2 = x %>% select(ano, mes_num, cod_mun, nombre_ciudad, codigo_articulo, articulo)
x2 = x %>% select(ano, mes_num, cod_mun, nombre_ciudad, codigo_articulo, articulo) %>% distinct()
View(x2)
x2 = x %>% select(cod_mun, nombre_ciudad, codigo_articulo, articulo) %>% distinct()
x2
View(x2)
writexl::write_xlsx(x2, "020925_dane_price_data.csv")
writexl::write_xlsx(x2, "020925_dane_price_data.xlsx")
length(levels(as.factor(x2$articulo)))
